# GWO using XG Boost
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
import xgboost as xgb

# Load the dataset
file_path = '/content/drive/MyDrive/sentiment_tweet.csv'
data = pd.read_csv(file_path)

# Preprocess the data
X_raw = data['message to examine']  # Text messages
y = data['label (depression result)']  # Labels

# Vectorize the text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=1000)  # Limit to 1000 features for simplicity
X = vectorizer.fit_transform(X_raw).toarray()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the objective function
def objective_function(weights):
    weights = weights.reshape(weights.shape[0], 1, weights.shape[1])
    accuracies = []
    for particle_weights in weights:
        X_train_weighted = X_train * particle_weights
        X_test_weighted = X_test * particle_weights

        # Train an XGBoost model without the use_label_encoder parameter
        model = xgb.XGBClassifier(eval_metric="mlogloss")
        model.fit(X_train_weighted, y_train)

        y_pred = model.predict(X_test_weighted)
        accuracy = accuracy_score(y_test, y_pred)

        accuracies.append(accuracy)
    return -np.array(accuracies)  # Negative for minimization
# GWO Implementation
def gwo(objective_function, dim, n_agents, max_iter, lb, ub):
    alpha_pos = np.zeros(dim)
    beta_pos = np.zeros(dim)
    delta_pos = np.zeros(dim)
    alpha_score = float("inf")
    beta_score = float("inf")
    delta_score = float("inf")
    positions = np.random.uniform(lb, ub, (n_agents, dim))
    for t in range(max_iter):
        for i in range(n_agents):
            positions[i, :] = np.clip(positions[i, :], lb, ub)
            fitness = objective_function(positions[i, :].reshape(1, -1))[0]

            if fitness < alpha_score:
                delta_score = beta_score
                delta_pos = beta_pos.copy()
                beta_score = alpha_score
                beta_pos = alpha_pos.copy()
                alpha_score = fitness
                alpha_pos = positions[i, :].copy()
            elif fitness < beta_score:
                delta_score = beta_score
                delta_pos = beta_pos.copy()
                beta_score = fitness
                beta_pos = positions[i, :].copy()
            elif fitness < delta_score:
                delta_score = fitness
                delta_pos = positions[i, :].copy()

        a = 2 - t * (2 / max_iter)
        for i in range(n_agents):
            for j in range(dim):
                r1, r2 = np.random.rand(), np.random.rand()
                A1 = 2 * a * r1 - a
                C1 = 2 * r2
                D_alpha = abs(C1 * alpha_pos[j] - positions[i, j])
                X1 = alpha_pos[j] - A1 * D_alpha

                r1, r2 = np.random.rand(), np.random.rand()
                A2 = 2 * a * r1 - a
                C2 = 2 * r2
                D_beta = abs(C2 * beta_pos[j] - positions[i, j])
                X2 = beta_pos[j] - A2 * D_beta

                r1, r2 = np.random.rand(), np.random.rand()
                A3 = 2 * a * r1 - a
                C3 = 2 * r2
                D_delta = abs(C3 * delta_pos[j] - positions[i, j])
                X3 = delta_pos[j] - A3 * D_delta

                positions[i, j] = (X1 + X2 + X3) / 3

        print(f"Iteration {t+1}, Best Fitness: {-alpha_score}")

    return alpha_pos, -alpha_score

def optimize_with_gwo():
    dim = X_train.shape[1]
    n_agents = 30
    max_iter = 50
    lb, ub = 0, 1
    best_weights, best_accuracy = gwo(objective_function, dim, n_agents, max_iter, lb, ub)
    return best_weights, best_accuracy

# Run GWO
print("\nOptimizing with GWO...")
best_weights_gwo, best_accuracy_gwo = optimize_with_gwo()

# Final evaluation of the selected weights
X_train_final = X_train * best_weights_gwo
X_test_final = X_test * best_weights_gwo

# Train the XGBoost model using the selected weights
final_model = xgb.XGBClassifier(eval_metric="mlogloss")
final_model.fit(X_train_final, y_train)

# Make predictions and calculate final accuracy
y_pred_final = final_model.predict(X_test_final)
final_accuracy = accuracy_score(y_test, y_pred_final)

# Print the final results
print("\nFinal Best Feature Weights:", best_weights_gwo)
print("Final Accuracy with Selected Weights:", final_accuracy)
